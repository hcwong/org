#+TITLE: CS3223
#+AUTHOR: Joshua Wong

* Lecture 1
** Problem with simple ASCII DBMS
Updates and deletes are expensive - for example, if we changed string from Cat to Cats, a full rewrite is needed.
ASCII storage is also expensive.

Searching is also expensive as we have no indexes. On that note, we also don't cache results, so if the same query is made, the system will just rerun the query.

Query processing is also difficult.

No concurrent control.

Reliability is also an issue as operations are not atomic - so if a crash occurs, the operation will be left half done.
Backups are also hard to do.

Security is also bad as the system is insecure and we don't have different level of privileges.

#+NAME: DBMS Architecture
#+ATTR_ORG: :width 300
[[file:images/cs3223_l1_1.png]]

** DBMS Store
DBMS Stores
1. Relations
2. Indexes
3. System Catalog (Relation Metadata)
4. Log files

** Memory Hierarchy
Primary memory is where the currently used data is stored, but the main database is stored on the secondary memory.

We cannot just use main memory to store everything as it is volatile and there is a larger chance of data corruption when we have a larger memory.
There are energy efficiencies issues as well.

** Data Access
Read and write operations are high cost as the processing is done in RAM, but info is stored on secondary memory.
Therefore, we need to read/write from the hard disks.

** Disks
#+NAME: Recap (Hard Disks)
#+ATTR_ORG: :width 300
[[file:images/cs3223_l1_2.png]]

Same as OS memory systems, data is stored and retrieved in pages and blocks (page and block used interchangeably in this module).
However, *DBMS manages its memory separate from OS*.
Note that the time taken to retrieve a disk page varies according to its relative location on disk at the time of access
- for example in HDD there is seek time (more significant) and rotational delay.

Therefore, blocks in a file should be arranged sequentially on disk to minimize seek and rotational delay.

** Disk Space Management (aka file system management)
1. Bitmap
2. Linked List
   Link all the free blocks together in a linked list

Other factors affecting allocation are granularity and allocation methods.

Smaller granularity results in fragmentation, larger one results in worse space utilization.

Allocation methods determine whether all the pages/blocks are close by (contiguously) or fragmented.

*** Managing space allocated to files
Within the file itself, they will manage the pages allocated to it by the DBMS. To the DBMS, these pages are already "allocated".
But from the file's point of view, they can see which ones are free and not.

Consists of a header page and data pages in a heap implementation.

#+NAME: Heap File Implementation
#+ATTR_ORG: :width 300
[[file:images/cs3223_l1_3.png]]

Alternatively, in a page directory, the header page just points to every data page, and can contain the number of free bytes for each page. 1 page contains 2 sectors (to check again), each sector counts as one I/O.

Buffer pool is the main memory allocated for DBMS and it is partitioned into pages called frames. We maintain a table of <frame#, pageid> pairs.
This allows for the application to check if the page has already been loaded.
Each frame also has two values, *pin count* (user count of the page in the frame) and *dirty flag*.

*** Requesting a Page
If page is not in buffer pool and if there are no free frames available, we choose a frame for replacement. If the frame is dirty, write it to disk.
Then, we read the requested page into the frame and increment pin count and return its address.
If requests can be predicted, we can also pre-fetch pages.

If no page can be replaced, we just delay the request.

*** Replacement Policies
1. FIFO - Good for seq access behavior.
2. LFU - Page with high reference activity will never been flushed.
3. LRU - Most commonly used, but this is bad when sequential flooding occurs.

*** Files of Records
High levels of DBMS operate on records and files of records.
A file is a collection of pages, each containing a collection of records. We want to support CRUD operations
on these records and files.
Usually, we want the record to fit within the page.

#+NAME: Record Formats.
#+ATTR_ORG: :width 300
[[file:images/cs3223_l2_1.png]]

#+NAME: Page format
#+ATTR_ORG: :width 300
[[file:images/cs3223_l2_2.png]]
Error in image above: Records cannot be shifted around within the page w/o changing the
record id.

Page formats can also be done in a variable length format. In this case, the slot directory
contains the size of each record and the number of records being stored in the page.
The benefit of the variable length format is that we can shift records around freely as
all we have to do is to move the pointer reference around.

* Lecture 2
** Indexes
Data structure on a file to speed up retrieval/selections based on some search key.
Every relation can have multiple search keys and hence multiple indexes.
The index is stored a as a file and records in an index file are referred to as data entries. Each record in the index
file can be visualized as a pair of <search key, pointer to data file for relation>.

We can also have multiple levels of index files to simplify queries.

#+NAME: Simple index file
#+ATTR_ORG: :width 300
[[file:images/cs3223_l2_3.png]]

Keeping all possible indexes may not be a very good idea as maintenance of the indexes is very expensive
and the number of possible indexes is exponential.

** Index Types
1. Tree based index - Based on sorting of search key values using B+ tree
2. Hash based index - Data entries access using hashing function

The criteria to evaluate index type
are search performance (equality and range search), storage overhead and update performance.

** Tree Structured Indexing
Excellent for equality and range based searching.

Every Relational DBMS uses B+ Tree.

#+NAME: B+ Tree
#+ATTR_ORG: :width 600
[[file:images/cs3223_l2_4.png]]

Key values for index entries are separators and may not correspond to any key values.

#+NAME: Order
#+ATTR_ORG: :width 600
[[file:images/cs3223_l2_5.png]]

*** Properties
1. Height Balanced
2. Update Efficient
3. Minimum 50% occupancy - Storage Efficient
4. Next leaf pointer (efficient range search)
5. Sorted leaf nodes

** Format of Data Entries
One format is k* is an actual data record.

The other is (k*, rid-list) where rid-list is a list of record identifiers of data records with search value k.
This is like our original format, except it is slightly more space efficient.

** Unclustered vs clustered index
An index is a *clustered index* if the order of its data is the same as the order of the data records. This means that
since the records are close together, the data is so, and maybe even in the same page, which has good efficiency.

To build a clustered index, we first have to sort the data file, with some free space on each page for future inserts.
Overflow pages may be needed for inserts (like linked list node chaining).

** Dense Index
There is at least one data entry (leaf) per search key value.
Sparse indexes just means that each entry may not have one search key value in the index.
Every sparse index is clustered and need to be sorted.

** Multi-attribute indexes
Composite search keys just mean that we are sorting based on a combination of fields.
#+NAME: Composite
#+ATTR_ORG: :width 300
[[file:images/cs3223_l3_1.png]]

Tip for searching: always pick the condition with a stricter requirement to aid in getting the results.

* Lecture 3: Hash-based Indexes
This is best for equality selections, but the performance degenerates for skewed data distributions
is usually inefficient for range searches.

** Static Hashing
Data is stored in M Buckets and M is fixed at creation time.

Each bucket consists of a primary data page and a chain of zero or more overflow data pages.
Primary pages are allocated sequentially and never de-allocated.

Ideally, the hash function you use should distribute the records over all the buckets to prevent overflow.
If there are long overflows, we now might need to change the hash function and reorganize.

** Dynamic Hashing
Linear hashing is a dynamic scheme and the hash file grows linearly (one bucket at a time) by splitting the buckets.

LH handles the problem of having long overflow chains. But there may still have overflow pages as the chain is
not immediately split.

Splitting a bucket is however an issue. When we obtain a new bucket B_j, we need to determine j, determine how to redistribute,
how to modify a hash function without modifying existing records.

#+NAME: Split Image
#+ATTR_ORG: :width 300
[[file:images/cs3223_l3_2.png]]

File size doubles after all the buckets have split according to the image above (one round of splitting).
To grow the file linearly, buckets are split sequentially. Bucket B_i must split before B_{i+1}. There is always a pointer
that helps to keep track to the next bucket that has yet to be split. We don't double the file after every split because we have to rehash everything instead of just one bucket => a lot of performance overhead.

When to split a bucket can be decided with various criteria, such as but not limited to:
1. Split whenever bucket overflows
2. Split when space utilization of file is above some threshold.

For our course, we assume that a bucket is triggered whenever *some* bucket overflows, aka all the primary and
overflow pages in the bucket are full.

For deletion, we can only reclaim a bucket if it is empty and if it is the last block.

We usually have 1 disk I/O if the bucket has no overflow pages. From testing, in LH we get an average of 1.2 disk I/O
for uniform or lowly skewed distribution. The worst case is the I/O cost is linear with the number of entries.
The space performance also does poorly if the data is skewed as per usual.

* Lecture 4
** Usefulness of Sorting
Usually the first step in bulk loading operations. It allows us to eliminate
duplicate copies in a collection of records and can help to evaluate join or set operations.

** External Sorting
Deals with the scenario where the data in disk cannot fit inside main memory.

We use the term *runs* to refer to the to subfiles that are to be combined and sorted. If there are enough buffers
for the runs, we can just merge it like merging sorted linked lists. If there are not enough buffers (with at least 3),
we can just combine files 2 at a time and then repeat the process until we get a final sorted file.

*** Multi-way Merge Sort
To sort a very large unsorted file, we can divide it into two steps, the first is to generate sorted runs and then
merge the sorted runs. The sorted runs can be generated by reading as many records as possible and using
in memory sort functions.

To sort a file with N pages using B buffer page, we can have \(\ceil{N/B}\) number of sorted runs with 1 pass (read + write, 2 * N I/Os).
To merge the sorted runs, we have B-1 buffer pages for input and 1 for output. We can do (B-1) way merge, each iteration
requires 1 pass (read + write), resulting in log_{b-1} \(\ceil{N/B}\) passes.

Hence, the cost of multi-way merge sort is 2 * N * (number of passes). In fact, increasing the number of buffer pages
has diminishing marginal returns wrt to reducing the number of passes.

*** Replacement Selection
Internal sort algorithms will generate \(\ceil{N/B}\) runs. If we want to have a longer run, we can use the replacement
selection algorithm.

#+NAME: Replacement Selection
#+ATTR_ORG: :width 500
[[file:images/cs3223_l4_1.png]]

The minimum length of a run is B, this occurs when the file is in descending order. This is the worst case. The best case
is when the file is in ascending order.

It's not always clear which method is better, on average, replacement works in practice though.

With increasing number of passes, the number of seeks decreases. For cost, the default is just to count I/O.
However, to distinguish, we will need to take into account seek time.

We can use clustered B+ tree for sorting but not unclustered index as the latter incurs one I/O per record on average.
For clustered it means that the cost of sorting is just traversing the index tree.
Unclustered may also be useful if buffer size is small as then the performance overhead will not be very big.
Unclustered may also save memory since we don't have to merge. Also, sorting is also a blocking operation, if we
only want a few pages, let's say top 10, from a resource consumption POV, its better to use unclustered.
