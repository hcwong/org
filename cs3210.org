#+TITLE: CS3210
#+AUTHOR: Joshua Wong

* Lecture 1

** Program Parallelization

Consists of three steps:
1. Decomposition of program into steps
2. Scheduling of assignment of tasks to processes
3. Map processes to cores


** Von Neumann Architecture
Consists of processor, memory, control scheme (fetch instruction and data)

However, there now exists the problem of /memory wall/, which is the speed disparity between the faster processor and the slower memory (time taken to fetch and interact with it)

*** Ways to improve performance
1. Higher clock frequency
2. Pipeline, doing things smater
3. Replication using multicore and cluster

** Parallel Computing

Defined as the *simulataenous use* of *multiple* processing units to solve a problem quickly.

Processing units start from core->processor->nodes->cluster. Note that processor and core may be used interchangably.

We usually try to find concurrent calculations and then run these on the different processing units.

*** Benefits
1. Overcome limits of serial computing
2. Save wall clock time
3. Solve larger problems
4. Improve performance
5. Use non-local or cheaper resources
6. Overcome memory constraint.

*** Evaluation
We evaluate based on execution time and throughput.

Parallel execution time is measured by computation time + overhead

** Computatational Model Attributes

1. Operation (Primitive Unit of computation)
2. Data
3. Control (scheduling the units of computation)
4. Communication (Shared memory vs distributed memory, ie thread vs processes)
5. Synchronization

** Challenges

The first is the granularity of the tasks (Amdahl's law), there are many ways to decompose the program.

Tasks may also depend on each other resulting in *data* or *control* dependencies. These impose execution order of parallel tasks.

*** Others
1. Compilers may either not optimize for parallel programming or not customize the parallelism for the hardware. This also applies to the runtime auto-tuners.
2. Locality
3. Load balance
4. Coordination
5. Debugging
6. Performance Modelling/Monitoring
* Lecture 2
** Parallel Architecture - Forms of Parallelism

*RECAP*: CPU time = Instructions / Program * Cycle / Instruction * Second / Cycle

*** Bit Parallelism
Increasing word size so as to reduce the number of instructions we have to do. For example, adding two 32 bit numbers with word size of 16 as opposed to just one instruction with 32 bit word size.

*** Instruction Parallelism
Has two types: Superscalar (space) and Pipelining (time) parallelism

**** Pipelining
Different instructions in different stages of the clock cycle.

The disadvantages are pipeline stalls, data and control flow issues
**** Superscalar
Involves duplicating the pipelining so as to allow multiple instructions to be in the same stage of the pipeline cycle.

But scheduling is challenging, as we have to decide which instructions to execute together => this scheduling can be done by the compiler (static) or hardware (dynamic).

Also, since multiple instructions can be in the same stage, when calculating CPU time, we prefer to take instructions epr cycle instead of cycle per instruction

*** Thread level Parallelism
Instruction level parallelism is limited due to data and control dependencies, so only 2-3 instructions can be executed at the same time.

Processor can provide hardware support with simulataenous multi threading => Diff PC, stack register in one core

#+TITLE: Multithreading implementation
#+ATTR_ORG: :width 300
[[file:images/cs3210_l2_1.png]]

*** Process level Parallelism
Needs some form of IPC and works by mapdpign the diferent processes to the diffferent cores.

** Flynn's Parallel architecture taxonomy.
Classified based on instruction stream and data stream

*** Single Instruction Single Data
Single instruction stream is executed by a single processing unit and each instruction works on a single data.
*** Single Instruction Multiple Data
Single instruction is applied to multiple data by multiple processing units. This is analogous to a map function.
*** Multiple Instruction Single Data
Multiple instructions being applied to a single data by multiple PUs. Doesn't exist IRL.
*** Multiple Instruction Multiple Data
Multiple PUs, each taking in their own independedent data stream and instruction stream. This is the basis of most multiprocessors
*** Variants
Variants of the above are possible. For example GPUs use a mixture of SIMD and MIMD.

** Multicore Architecture
*** Hierachial Design
Multiple cores share multiple caches. As the caches get further from the core, they get bigger, and slower. Example of this is the L1 L2 cache structure.
*** Pipelined Design
Data elements are perocessed by multiple cores in a pipelined manner. All the cores are connected to a shared cache and memory.
This is useful when some sequence of computation needs to be done on a data.
*** Network Based Design
Cores and their local cache and memory are independent units, but are connected via some interconnection network control.

** Memory Organization
#+TITLE: Memory Organization
#+ATTR_ORG: :width 300
[[file:images/cs3210_l2_2.png]]

*** Distributed Memory System
Each node, consisting of processor, cache and memory are independent and the memory is private to each node. Data exchange between nodes take place via message passing (expensive) through a network.
Memory is *not* shared.
*** Shared Memory System
Parallel programs access threads via a shared memory provider. Actual hardware memory architecture is abstracted away.
Data exchange takes place via shared variables.

The disadvantages of these are memory consistency and cache coherence issues. If one core changes a shared variable, we have to ensure that the rest of the cores have the same consistent view of the memory, and that their caches must be the same. This is usually done by hardware. There is also the issue of contention as there may be a limit to how many processes can simultaenous access memory due to issues like I/O or read port overcrowding.

Two other factors affect shared memory systems: Processor to memory delay and presence of a local cache with cache coherence protocol.

| Advantages                                             | Disadvantages                     |
|--------------------------------------------------------+-----------------------------------|
| - No need to partition code or data                    | - Need synchronization constructs |
| - No need to move data amongst processors => efficient | - Contention                      |

**** UMA
Latency of accessing main memory is uniform as memory is shared, though runs the risk of contention
**** NUMA
Physical memory is distributed across the different nodes (but still shared!), though now accessing other node's memory takes far longer (non-uniform) than accessing local memory.
**** Cache Coherent NUMA
Each cache maintains the cache coherence between the different processors.
**** Cache Only Memory Architecture
Each memory block works as cache memory and the blocks are indpendent. Data migrates dynamically and continously according to coherence scheme.
**** Hybrid
#+TITLE: Hybrid distributed-shared architecture
#+ATTR_ORG: :width 300
[[file:images/cs3210_l2_3.png]]
* Lecture 3
Parallelism - avg units of work (tasks + dependencies) that can be performed in parallel per unit time

Limits of parallelism include data and control dependencies, comm && thread/process overhead, synchronization, contention

| Data Parallelism                                                                       | Task Parallelism    |
|----------------------------------------------------------------------------------------+---------------------|
| Partition the data so each processing unit carries out same operations on part of data | Partition the tasks |

** Data Parallelism
Good when the operations are independent - also supported by libraries like OpenMP's pragma statements.

Common Model : Single Program Multiple Data where a parallel program is executed by all processors in parallel.

** Task Parallelism
Can use a task dependence graph to visualize the task decomposition strategy. This is a DAG, where the node represents each task (node val is task execution time) and the edge a control dependency.
Properties of the task dependence graph include critical length path (slowest time) and degree of concurrency (tota task/critical path length)

** Representation of Parallelism
#+ATTR_ORG: :width 300
[[file:images/cs3210_l3_1.png]]

** Models of Communication
*** Shared Address Space
- Communication via shared variables
- Ensure mutual exclusion via locks
- Logical extension of uniprocessor programming
However, this usually requires hardware support to work efficiently. Even with NUMA, it is still relatively costly to scale.
*** Data Parallel
Same operation on each element of an array => no communication between distinct function invocations. This model is analogous to stream programming, but it is very restrictive.
*** Message Parsing
Tasks have their own private address space, matches distributed memory systems.
*** Correspondence with hardware implementations
In general, there isn't a 1-1 correspondence per se, but it is easier to mimic message parsing with shared memory hardware systems than vice versa.

** Program Parallelization
First we need to define the tasks of an apt granularity, along with their dependencies.

Task/Channel Model consists of the code and data for parallelization mapped onto processors and they communicate via message buffers (channels)
*** Foster's Design Methodology
**** Partitioning
We can divide it into data or task parallelism.

Paritioning Rules Of Thumb

1. At least 10x more primitive tasks than processors in target machine
2. Minimize redundant computations and data storage
3. Primitive tasks of roughly the same size
4. Number of tasks proportional to problem size
**** Communication
Local communication where tasks only needs data from small subset of other tasks. Global if it needs values from a lot of other tasks.

Rules of Thumb
1. Communication operations balanced out.
2. Each tasks communicates locally mostly
3. Communication can be done in parallel.
4. Overlap computation and communication.

**** Agglomeration
Combine tasks into larger tasks, but still number of tasks >= cores. The core idea is to reduce the number of communication.

Rules of Thumb
1. Communication should decrease
2. Number of tasks still proportional to problem size
3. Still 10x more tasks than cores
4. Tradeoff between agglomeration and code modification.
**** Mapping
Maximize processor utilization, but minimize inter processor communication.

Rules of Thumb
1. Heuristic to get optimal mapping
2. Design tasks based on one task per processor and multiple task per processor.
3. If dynamic task allocator, task allocator should not be a perf bottleneck. Static allocator should still follow 10:1 rule
*** Decomposition
Generate enough tasks to keep cores busy at all times and larger than overhead -> can be static or dynamic.
*** Scheduling
Find efficient task execution order -> can be static or dynamic.
*** Mapping
Assignment of processes and threads to execution units -> Focus on performance by minimizing communication and utilizing all cores.
*** Automatic Parallelization
Letting compilers decompose and schedule.
But dependence analysis is hard with pointer based computation or indirect addressing, execution time of calls or loops with unknown bounds also hard.
*** FP languages
Benefit is that the functions are already independent, but we have to figure out when at which function level we want to extract parallelism.
