#+TITLE: CS3210
#+AUTHOR: Joshua Wong

* Lecture 1

** Program Parallelization

Consists of three steps:
1. Decomposition of program into steps
2. Scheduling of assignment of tasks to processes
3. Map processes to cores


** Von Neumann Architecture
Consists of processor, memory, control scheme (fetch instruction and data)

However, there now exists the problem of /memory wall/, which is the speed disparity between the faster processor and the slower memory (time taken to fetch and interact with it)

*** Ways to improve performance
1. Higher clock frequency
2. Pipeline, doing things smater
3. Replication using multicore and cluster

** Parallel Computing

Defined as the *simulataenous use* of *multiple* processing units to solve a problem quickly.

Processing units start from core->processor->nodes->cluster. Note that processor and core may be used interchangably.

We usually try to find concurrent calculations and then run these on the different processing units.

*** Benefits
1. Overcome limits of serial computing
2. Save wall clock time
3. Solve larger problems
4. Improve performance
5. Use non-local or cheaper resources
6. Overcome memory constraint.

*** Evaluation
We evaluate based on execution time and throughput.

Parallel execution time is measured by computation time + overhead

** Computatational Model Attributes

1. Operation (Primitive Unit of computation)
2. Data
3. Control (scheduling the units of computation)
4. Communication (Shared memory vs distributed memory, ie thread vs processes)
5. Synchronization

** Challenges

The first is the granularity of the tasks (Amdahl's law), there are many ways to decompose the program.

Tasks may also depend on each other resulting in *data* or *control* dependencies. These impose execution order of parallel tasks.

*** Others
1. Compilers may either not optimize for parallel programming or not customize the parallelism for the hardware. This also applies to the runtime auto-tuners.
2. Locality
3. Load balance
4. Coordination
5. Debugging
6. Performance Modelling/Monitoring
* Lecture 2
** Parallel Architecture - Forms of Parallelism

*RECAP*: CPU time = Instructions / Program * Cycle / Instruction * Second / Cycle

*** Bit Parallelism
Increasing word size so as to reduce the number of instructions we have to do. For example, adding two 32 bit numbers with word size of 16 as opposed to just one instruction with 32 bit word size.

*** Instruction Parallelism
Has two types: Superscalar (space) and Pipelining (time) parallelism

**** Pipelining
Different instructions in different stages of the clock cycle.

The disadvantages are pipeline stalls, data and control flow issues
**** Superscalar
Involves duplicating the pipelining so as to allow multiple instructions to be in the same stage of the pipeline cycle.

But scheduling is challenging, as we have to decide which instructions to execute together => this scheduling can be done by the compiler (static) or hardware (dynamic).

Also, since multiple instructions can be in the same stage, when calculating CPU time, we prefer to take instructions epr cycle instead of cycle per instruction

*** Thread level Parallelism
Instruction level parallelism is limited due to data and control dependencies, so only 2-3 instructions can be executed at the same time.

Processor can provide hardware support with simulataenous multi threading => Diff PC, stack register in one core

#+TITLE: Multithreading implementation
#+ATTR_ORG: :width 300
[[file:images/cs3210_l2_1.png]]

*** Process level Parallelism
Needs some form of IPC and works by mapdpign the diferent processes to the diffferent cores.

** Flynn's Parallel architecture taxonomy.
Classified based on instruction stream and data stream

*** Single Instruction Single Data
Single instruction stream is executed by a single processing unit and each instruction works on a single data.
*** Single Instruction Multiple Data
Single instruction is applied to multiple data by multiple processing units. This is analogous to a map function.
*** Multiple Instruction Single Data
Multiple instructions being applied to a single data by multiple PUs. Doesn't exist IRL.
*** Multiple Instruction Multiple Data
Multiple PUs, each taking in their own independedent data stream and instruction stream. This is the basis of most multiprocessors
*** Variants
Variants of the above are possible. For example GPUs use a mixture of SIMD and MIMD.

** Multicore Architecture
*** Hierachial Design
Multiple cores share multiple caches. As the caches get further from the core, they get bigger, and slower. Example of this is the L1 L2 cache structure.
*** Pipelined Design
Data elements are perocessed by multiple cores in a pipelined manner. All the cores are connected to a shared cache and memory.
This is useful when some sequence of computation needs to be done on a data.
*** Network Based Design
Cores and their local cache and memory are independent units, but are connected via some interconnection network control.

** Memory Organization
#+TITLE: Memory Organization
#+ATTR_ORG: :width 300
[[file:images/cs3210_l2_2.png]]

*** Distributed Memory System
Each node, consisting of processor, cache and memory are independent and the memory is private to each node. Data exchange between nodes take place via message passing (expensive) through a network.
Memory is *not* shared.
*** Shared Memory System
Parallel programs access threads via a shared memory provider. Actual hardware memory architecture is abstracted away.
Data exchange takes place via shared variables.

The disadvantages of these are memory consistency and cache coherence issues. If one core changes a shared variable, we have to ensure that the rest of the cores have the same consistent view of the memory, and that their caches must be the same. This is usually done by hardware. There is also the issue of contention as there may be a limit to how many processes can simultaenous access memory due to issues like I/O or read port overcrowding.

Two other factors affect shared memory systems: Processor to memory delay and presence of a local cache with cache coherence protocol.

| Advantages                                             | Disadvantages                     |
|--------------------------------------------------------+-----------------------------------|
| - No need to partition code or data                    | - Need synchronization constructs |
| - No need to move data amongst processors => efficient | - Contention                      |

**** UMA
Latency of accessing main memory is uniform as memory is shared, though runs the risk of contention
**** NUMA
Physical memory is distributed across the different nodes (but still shared!), though now accessing other node's memory takes far longer (non-uniform) than accessing local memory.
**** Cache Coherent NUMA
Each cache maintains the cache coherence between the different processors.
**** Cache Only Memory Architecture
Each memory block works as cache memory and the blocks are indpendent. Data migrates dynamically and continously according to coherence scheme.
**** Hybrid
#+TITLE: Hybrid distributed-shared architecture
#+ATTR_ORG: :width 300
[[file:images/cs3210_l2_3.png]]
* Lecture 3
Parallelism - avg units of work (tasks + dependencies) that can be performed in parallel per unit time

Limits of parallelism include data and control dependencies, comm && thread/process overhead, synchronization, contention

| Data Parallelism                                                                       | Task Parallelism    |
|----------------------------------------------------------------------------------------+---------------------|
| Partition the data so each processing unit carries out same operations on part of data | Partition the tasks |

** Data Parallelism
Good when the operations are independent - also supported by libraries like OpenMP's pragma statements.

Common Model : Single Program Multiple Data where a parallel program is executed by all processors in parallel.

** Task Parallelism
Can use a task dependence graph to visualize the task decomposition strategy. This is a DAG, where the node represents each task (node val is task execution time) and the edge a control dependency.
Properties of the task dependence graph include critical length path (slowest time) and degree of concurrency (tota task/critical path length)

** Representation of Parallelism
#+ATTR_ORG: :width 300
[[file:images/cs3210_l3_1.png]]

** Models of Communication
*** Shared Address Space
- Communication via shared variables
- Ensure mutual exclusion via locks
- Logical extension of uniprocessor programming
However, this usually requires hardware support to work efficiently. Even with NUMA, it is still relatively costly to scale.
*** Data Parallel
Same operation on each element of an array => no communication between distinct function invocations. This model is analogous to stream programming, but it is very restrictive.
*** Message Parsing
Tasks have their own private address space, matches distributed memory systems.
*** Correspondence with hardware implementations
In general, there isn't a 1-1 correspondence per se, but it is easier to mimic message parsing with shared memory hardware systems than vice versa.

** Program Parallelization
First we need to define the tasks of an apt granularity, along with their dependencies.

Task/Channel Model consists of the code and data for parallelization mapped onto processors and they communicate via message buffers (channels)
*** Foster's Design Methodology
**** Partitioning
We can divide it into data or task parallelism.

Paritioning Rules Of Thumb

1. At least 10x more primitive tasks than processors in target machine
2. Minimize redundant computations and data storage
3. Primitive tasks of roughly the same size
4. Number of tasks proportional to problem size
**** Communication
Local communication where tasks only needs data from small subset of other tasks. Global if it needs values from a lot of other tasks.

Rules of Thumb
1. Communication operations balanced out.
2. Each tasks communicates locally mostly
3. Communication can be done in parallel.
4. Overlap computation and communication.

**** Agglomeration
Combine tasks into larger tasks, but still number of tasks >= cores. The core idea is to reduce the number of communication.

Rules of Thumb
1. Communication should decrease
2. Number of tasks still proportional to problem size
3. Still 10x more tasks than cores
4. Tradeoff between agglomeration and code modification.
**** Mapping
Maximize processor utilization, but minimize inter processor communication.

Rules of Thumb
1. Heuristic to get optimal mapping
2. Design tasks based on one task per processor and multiple task per processor.
3. If dynamic task allocator, task allocator should not be a perf bottleneck. Static allocator should still follow 10:1 rule
*** Decomposition
Generate enough tasks to keep cores busy at all times and larger than overhead -> can be static or dynamic.
*** Scheduling
Find efficient task execution order -> can be static or dynamic.
*** Mapping
Assignment of processes and threads to execution units -> Focus on performance by minimizing communication and utilizing all cores.
*** Automatic Parallelization
Letting compilers decompose and schedule.
But dependence analysis is hard with pointer based computation or indirect addressing, execution time of calls or loops with unknown bounds also hard.
*** FP languages
Benefit is that the functions are already independent, but we have to figure out when at which function level we want to extract parallelism.
* Lecture 4
** Models of Parallelism
*** Fork-Join
T creates child tasks and then waits for the tasks using a join call
*** Parallel Begin Parallel End
Programmer specifies a sequence of statement to be executed by a set of processors in parallel.
The statements following the parbegin-parend construct are only executed after threads finished their work
*** SIMD
Single instructions are executed *synchronously* by the different threads on different data.
*** SPMD
 Same program executed on different processes on different data. All threads have equal rights and work asynchronously with each other.
*** Master-Slave
 A single program master controls the execution of the program and assigns work to slave threads.
*** Client Server
 Multiple Program Multiple Data model

 The client sends requests, which are tasks for the server to compute, and to return the result to the client.
*** Task Pool
 Data structure where threads can access to retrieve tasks for execution.

 Number of threads are fixed. This is an improvement over client server as we won't spawn too many threads now and overhead of thread creation is independent of the problem size.

 Access to the task pool must be synchronized to avoid race conditions.
*** Producer Consumer
 Producer produces data which is consumed by the consumer.
*** Pipelining
Data ios partitioned into a stream that flows through each of the pipeline tasks.

** Performance
Two Goals: Users care about *reduced response time* but computer managers want *high throughput*

*** User CPU Time
User CPU Time of Program = Total Number of CPU cycles for instruction * Time per cycle - (1)

But instructions have *diff execution times* (from CS2100). So the total number of CPU cycles is the number of instructions of a type * the average number of CPU cycles for a type of instruction.

*** Refinement with Mem Acesss Time
We also have to add in the additional clock cycles due to memory access or if access the larger caches.

It can further be extended by adding in readwrite miss rate,
giving \(Time_{user}(A) = (N{instr}(A) * CPI(A) + N_{rw_op}(A) * RW_{miss}(A) * N_{miss_cycles}) * Time_{cycle} \)

*** Average Memory Access Time
Mem access time = (read hit time) + Percentage of Read Miss * (read miss time)

Note that the miss time is on top of the read hit time.

*** Throughput - Million Instructions per second
Two ways to calculate.
1. Number of Instructions / user cpu time / 10^6
2. Clock Frequency / CPI(A) / 10^6

** Speedup
\(T_p(n)\) where p stands for number of processors, T is the end time - start time of the program on all processors.

Cost of parallel program with input size n on p processors is \(C_p(n) = p * T_p(n)\)

A parallel program is *cost optimal* if it executes the same number of operations as the fastest sequential program.

\(S_p(n) = T_{best_seq}(n) / T_p(n)\). In practice, superlinear speedup can occur if problem fits entirely in cache.

But, fastest speedup is hard to know. So we just use \(E_p(n) = T_*(n) / p / T_p(n)\). Ideally, \(S_p(n) = p\)

** Scalability
Interaction behind size of problem and size of parallel comp. Ideally, we want to match the problem size and the size of the machine (eg large problem and small machine leads to thrashing.)

*** Scaling Constraints
Application-oriented scaling problems
Resource-oriented scaling properties:
1. Problem constraint scaling - Use parallel computer to solve faster
2. Time problem scaling - Complete more work in fixed time
3. Memory constrained scaling - Run largest problem w/o overflowing main mem.

** Amdahl's Law
Speedup of the parallel execution is limited by the fraction (f) of the algorithm that cannot be parallelized.

\(S_p(n) \leq 1/f\)

However, for in many problems, /(f)/ is not a constant and speedup can reach p, the number of processors.

** Gustafson's Law
Certain applications have execution time as main constraint => /f/ decreases as problem size increases.

\(S_p(n) \leq p\)

We can put in more resources to get the same execution time as sequential so that the speedup can tend to p.

** Performance Analysis
1. Set the Goals: Throughout, response time?
2. Start with simplest parallel solution
3. Check possible bottlenecks: Instruction, Memory, Locality of Data access, sync overhead
* Lecture 6
General Purpose GPU(GPGPUs) are used noawadays, like CUDA and OpenCL.
** GPU Architecture
Multiple Streaming Processors (SMs) with their own memory and L1 cache. They consist of multiple compute cores.

The compute capabilities differ across the versions from 1x to 7x. We focus on Volta (7.0, 7.2) and Turing (7.5).

** CUDA Programming
Stands for compute unified device architecture.

It comes with a general purpose programming model, that is a simple extension to standard C; it is however not a graphics API like openGL. It focuses more on how you write your program.

CUDA works on Nvidia GPUs and comes with hardware architecture, programming model and programming interface.

- Parallel portions execute on device(GPU) as kernels(functions that run on device).
- CUDA threads are extremely lightweight => good for context switching

A CUDA kernel is executed by array of threads (thread block) that execute same code (SPMD). Each thread that has ID to compute mem addr and make control decisions.
The threads execute in lock step (maximum of 32, determined by WARP scheduler). However, if there is branching, this no longer holds and different branch flows will execute sequentially.

The threads are also not completely independent and they can share memory. This cooperation includes shared mem, atomic operations and synchronization is available to threads of the same block.
A kernel is executed by a grid of thread blocks. The thread ID determines what data the thread works on.

*** Execution Model Mapping To Architecture
Kernels are launched in grids and can execute concurrently.

A block usually executes on one SM, and does not migrate. Also possible for serveral blocks to reside concurrently on SM.
*** Thread Execution Mapping to Architecture
SIMT execution model.

Multiprocessor creates and executes threads in SIMT warps => the threads in it start together at same prog addr.
Most of the time, a block is split into warps the same way.

/Blocks vs Warps: Blocks is a conceptual unit of work, but warp is the hardware group/

** CUDA Memory Model
#+ATTR_ORG: :width 400
[[file:images/cs3210_l6_1.png]]

Local memory is very slow, so we try to store info in thread registers. Block shared mem has decent speed, but global memory has relatively slow access times.

Global memory is able to communicate with the host (CPU), though transferring the data has overhead.

Shared memory should be used as much as possible due to its higher bandwidth and lower latency.
The job of a programmer is to ensure that each thread of the block access a different part of the shared memory (banks), as differnet banks can be accessed simultaenously. Avoid bank conflicts!

For coalesced access to global memory, simultaenous accesses to global memory in a warp can be coalesced into a single transaction of 32, 64, of 128 bytes.
Note that the memory access need not be contigous, they just need to be in the same segment.

** CUDA Software Development
The CUDA C runtime is a minimal set of extensions to C Language and requires a runtime API.

The CUDA driver API loads compiled kernels, inspect parameters and to launch them (not the same as runtime API).

#+ATTR_ORG: :width 400
[[file:images/cs3210_l6_2.png]]

Need to use NVCC, a compiler driver for CUDA => produces C code for CPU and PTX.

** CUDA Programming
No static variables, variable nummber of args and no recursion.

The function qualifiers determine where the function runs (host vs device vs global, or host and device together).

CUDA also allows us to delegate memory placement using /__managed__/, so we can use the unified memory model w/o data transfer.

/void __syncthreads();/ also helps to synchronize all threads in a block => do after each shared mem operation.

** Optimizing CUDA Programs
1. Maximize Parallel Execution
   - Expose as much data parallelism => threads are cheap!
   - More thread blocks than multiprocessors
   - Threads per block should be multiple of warp size and 64 to avoid mem bank conflict.
2. Optimize mem usage to achieve max bandwidth
   - Minimize Data Transfer
3. Optimize instruction usage, avoid stuff like branching.
   - Minimize divergent warps caused by control flow instructions.
