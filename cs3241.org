#+TITLE: CS3241
#+AUTHOR: Joshua Wong

This course focuses on 3D computer graphics, but graphics deals with creating images with a computer, using applications, software and hardware.

* Lecture 1
** Display Processor
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_1.png]]

Expensive to have host computer refresh display, so use special purpose computer called display processor (DPU)

Graphics stored in display list on display processor
Host compiles display list and sends to DPU

** Raster Graphics and OpenGL
Image produced as an array (the raster) of pixels in frame buffer

Then came OpenGL API after that

** Image Formation
In computer graphics, 2D images are formed like how they would in cameras and eyes.

*** Elements of Image Formation
1. Objects
2. Light
   - For humans, only care about RGB for luminance images. For monochromatic, only values are grey values
   - Don't have to match all frequency in visible spectru because of cones only take in RGB
   - There are two types of color: Additive and Subtractive. Additive is formed via RGBs. Subtractive is via filtering white light with Cyan, Magenta and Yellow
3. Viewer
4. Material

#+NAME: Pinhole camera
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_2.png]]

#+NAME: Synthetic camera
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_3.png]]

This paradigm looks at creating a computer generated image as being similar to forming an image using an optical system.

*** Advantages of Synthetic Camera system
The advantages of such synthetic camera model is the separation of objects viewers and light sources -> simpler API.

Image is also not moved to infront of the camera

** Lighting
Lighting of objects cannot be calculated independently as light can reflect, and some objects are also blocked from light or are translucent.

** Image Formation Model

The API mimics the synthetic camera model to design graphics hardware & software.

For the API, we only need to specify: Objects, materials, viewer and lights

*** Physical approaches
Ray Tracing - follow rays of light from center of projection until they are absorbed by objects or go off course. It can handle global effects but is slow and must keep whole database available

Radiosity - Energy based approach that is slow and not general.

*** Practical Approach for Real-Time rendering
#+NAME: Pipeline architecture
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_4.png]]

All this steps can be implemented in graphics hardware

**** Vertex Processing
Converting object representations from one coordinate system to another (object, synthetic camera, screen coords) via matrix transformations. This is because our 3D scene is described by polygons, which in turn is described by their vertices.

Vertex processor also computes lighting.

**** Projection
This is the process that combines the 3D viewer with the 3D objects to produce 2D image.

Two kinds: Perspective projection where all projectors meet at the center of the projection.
Parallel where the projectors are parallel and center of projection is replaced with a direction of projection.

Part of projection is done by the vertex processor.

**** Primitive Assembly
Vertices must be collected into geometric objects and primitives before clipping and rasterization can take place. So stuff like lines, polygons, curves

**** Clipping
A camera can only see the part of the world within the object space, so things not within this space are clipped out via a series of panes via modifying of the polygons if necessary.

#+NAME: Clipping
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_5.png]]

**** Rasterization
Applies color to the appropriate pixels in the frame buffer aka filling in the interior.

For each object, the rasterizer produces a set of *fragments* for each object/primitive. *Fragments* are potential pixels and they have a location in frame buffer, color and depth attributes.

Vertex attributes are interpolated over the vertex objects by the rasterizer

**** Fragment Processing
Fragments are processed to determine color of corresponding pixel in frame buffer. Colors are determined by *texture mapping* or interpolation of vertex colors.

Fragments can also be blocked by fragments closer to camera and this is removed (*hidden surface removal*)

*** Programmer's interface
This system is exposed via the graphics library API

It exposes functions that specify what is needed to form an image

- Objects
- Viewer
- Light Source
- Material
- Other info like device input and system capabilities

APIs also support a limited set of primitives like points (0D), lines (1D), polygons (2D), curves and surfaces. All these are defined through locations in space or vertices.

#+BEGIN_SRC cpp
glBegin(GL_POLYGON); // type of object
  glVertex3f(0.0, 0.0, 0.0); // location of vertex
  glVertex3f(0.0, 1.0, 0.0);
  glVertex3f(0.0, 0.0, 1.0);
gl.End(); // end of obj definition
#+END_SRC

*** Camera Specification
#+NAME: Camera Specification
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_6.png]]

*** Lights and Materials
Types of light and material property (absorption, diffuse vs specular scattering) also matter

* Lecture 2 :OpenGL:
OpenGL is a library that mainly focuses on rendering.

It however does not have windowing, support for window systems was added by GLUT.

** Open GL Functions
- Primitives (Points, lines, polygons)
- Attributes
- Transformations (Viewing, Modelling)
- Control (GLUT)
- Input (GLUT)
- Query
 
However, it lacks OOP support so there are often multiple functions that do the same thing.

#+BEGIN_SRC cpp
// All the same
glVertex3f
glVertex2i
glVertex3dv
#+END_SRC

** State
OpenGL is a state machine and exposes two types of functions:
1. Primitive generating - Generates new output if primitive visible
2. State changing via transformation or attribute functions.

Attributes like color, size, polygon mode are part of state and affect object appearance.

** Sample Program

#+TITLE: Square on Solid background
#+BEGIN_SRC cpp
#include <GL/glut.h>

void mydisplay() {
  glClear(GL_COLOR_BUFFER_BIT);
  glBegin(GL_POLYGON);
    glVertex2f(-0.5, -0.5);
    glVertex2f(-0.5, 0.5);
    glVertex2f(0.5, 0.5);
    glVertex2f(0.5, -0.5);
  glEnd();
  glFlush();
}

int main(int argc, char **argv) {
  glutCreateWindow("simple");
  glutDisplayFunc(mydisplay);
  glutMainLoop();
}
#+END_SRC

Program defineds a display callback function mydisplay. Every GLUT program must have display callback and it is executed
everytime the display is refreshed. Main function ends with program entering *event loop*.

However, a better structure would involve having 3 functions:
1. main() - Defines callbacks, open windows, enters event loop
2. init() - Sets state variables like attributes
3. callbacks - display callback function, have input and window functions

*NB:* glOrtho(x1, x2, y1, y2, z1, z2) defines the range of viewing volume, used for clipping.

** Coordinate systems
The units in /glVertex/ are determined by application and are called object coordinates. OpenGL will convert world coords to camera coords and then to window coords

OpenGL camera is at worldspace origin, looking in negative z-direction. Viewing volume is default centered at origin with side of length 2.
In the default orthographic view, points are projected forward along the z axis onto the plane z = 0.

Apart from windows, we can use smaller *viewports*, like /glViewport(x, y, w, h)/. Values are in window coords.

** Transformations and viewing
In OpenGL, projection is carried out by the projection matrix.

** Polygon issues
OpenGL only correctly display polygons if simple, convex (all points on line segment between two points in polygon are in polygon), and are flat.

Triangles satisfy all conditions. If not satisfy, may display incorrectly.

** RGB color
Each color component (R / G / B) is stored separately in frame buffer, usually with 8 bits. Color is part of state and is not part of object, rather assigned when object is rendered.

* Lecture 3
** 3D applications
In OpenGL, 2D is just a special case of 3D so there are not too many changes, just use the glVertex3*() series functions..

** Sierpinski Gasket
Start with a triangle, connect bisector of sides and remove central triangle.
#+TITLE: Sierpinski
#+ATTR_ORG: :width 300
[[file:images/cs3241_l3_1.png]]

However, as we continue subdividing, area goes to 0 and perimeter goes to infinity. This is a fractional dimension object, known as a fractal.

3D Gasket can be done by subdividing each of the four faces. Appears as if we remove solid tetrahedron from the center leaving 4 smaller tetrahedra.

** Hidden Surface Removal
OpenGL uses a z-buffer algorithm to save depth information as objects/polygons are rendered.

** Graphical Input
Devices can be described by physical properties like mouse and keyboard and logical properties like what is returned to the program via API.

*** Incremental Devices
Devices such as the data tablet return position directly to operating system, whereas mouse and all return incremental inputs.

** Trigger and Measure
Input devices contain a trigger which can be used to send a signal to the OS, and it returns information (their measure) to the system.

** Event Mode
Each trigger generates an event whose measure is put into the event queue for later processing. We can then define
callback functions and tag it to a certain event to respond accordingly.

** Posting Redisplay
Many events implicitly invoke the display callback function. To avoid spamming it in a single event loop, we can use the /glutPostRedisplay()/ function.

** Animating a display
When redrawing, we clear the window then draw. But the drawing of info in frame buffer is decoupled form display => result in partially drawn display.

Solution is to use two color buffers, and swap whenever necessary.

** Positioning
Y coordinate of mouse cursor position and OpenGL are different. To convert: y_{opengl} = h - 1 - y_{win}.

Note that to invert the y position we need the window height => this can be tracked with a global variable.

** Reshaping window
Two possibilities: Display part of world or display whole world but force to fit in new window. OpenGL exposes a reshape callback function.

Reshape callback is a good place to put the viewing function because it is invoked when the window is first opened.

We tend to return to modelview mode after the reshape callback.

* Lecture 4
Geometry is the study of spatial relationships among objects in n-dimensional space.
We use 3 primitives - scalars, vectors, points from which we can build more sophisticated objects.

** Affine Space
Because vectors don't have a position, only direction, we join a point and a vector space to define an affine space.

*** Rays and Line Segements
If \(\alpha >= 0\), then \(P(\alpha)\) = P_0 + \(\alpha . d\), where \(P(\alpha\) is the ray leaving the point P_0 in direction d.

** Representation
Need a frame of reference to relate points and objects to physical world.

*** Coordinate systems
Consider a basis v_1, v_2, ..., v_n. (set of linearly independent vector).
A vector is written v = a_1.v_1 + ... + a_n.v_n. The list of scalars {a_1, ..., a_n} is the representation of v wrt given basis.

*** Frames
Coordinate system insufficient to represent points. In affine space, we add origin to basis vectors to form a frame.
Thus the frame is determined by (P_0, v_1, v_2, v_3).

*** Homogenous Coordinates
Let \(x\) be \([v_1, v_2, v_3, P_0]\).
A vector will be represented as \(x . [a_1, a_2, a_3, 0]^T\).

A point on the other hand will be represented as \(x . [b_1, b_2, b_3, 1]^T\)

Usually however we omit the \(x\).

Homogenous coordinates are important as now all transformations can be implemented with matrix multiplications using 4x4 matrices.

*** Object, World and Camera Frames
We work with n-tuples all arrays of scalar when working with representation. In OpenGL, vertices specified in object frame. Vertices then represented in world frame. Then entities are represented in the camera frame using model view matrix.

** Transformation
Rotations, Translation, Scaling, Shear. The former 2 are rigid body transformations (preserve shape).

*** Affine Transformations
- Are line preserving

*** Translation
#+NAME: Translation Matrix
#+ATTR_ORG: :width 400
[[file:images/cs3241_l4_1.png]]

*** Rotation
- \(x' = x.cos(\theta) - y.sin(\theta)\)
- \(y' = x.sin(\theta) - y.cos(\theta)\)
- \(z' = z\)

#+NAME: Rotation
#+ATTR_ORG: :width 300
[[file:images/cs3241_l4_2.png]]

#+NAME: Rotation Matrix
#+ATTR_ORG: :width 300
[[file:images/cs3241_l4_3.png]]

*** Scaling and Reflection
Involves multiplying the values by either the scale factor or 1/-1.

*** Inverse
- Translation: \(T^{-1}(d_x, d_y, d_z) = T(-d_x, -d_y, -d_z)\)
- Rotation: \(R^{-1}(\theta) = R^T(\theta)\)
- Scaling: \(S^{-1}(s_x, s_y, s_z) = S(1/s_x, 1/s_y, 1/s_z)\)

*** Concatenation
Can form arbitray affine transformation matrices by multiplying rotation transformation scaling matrices.

*** Shear
Pulling faces in opposite directions

** OpenGL Matrices
Current Transformation Matrix is a 4x4 matrix applied to all vertices in the pipleine. We change CTM by loading a new one or by post-multuplication.

OpenGL has a model-view and projection matrix in the pipeline. Each has a CTM and can be manipulated indpendently.

Model-view matrix is use to transform objects to worldspace and position camera. Projection matrix is used to define the view volume.

*** Matrix stacks
Allow us to save transformation matrices for use later.
