#+TITLE: CS3241
#+AUTHOR: Joshua Wong

This course focuses on 3D computer graphics, but graphics deals with creating images with a computer, using applications, software and hardware.

* Lecture 1
** Display Processor
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_1.png]]

Expensive to have host computer refresh display, so use special purpose computer called display processor (DPU)

Graphics stored in display list on display processor
Host compiles display list and sends to DPU

** Raster Graphics and OpenGL
Image produced as an array (the raster) of pixels in frame buffer

Then came OpenGL API after that

** Image Formation
In computer graphics, 2D images are formed like how they would in cameras and eyes.

*** Elements of Image Formation
1. Objects
2. Light
   - For humans, only care about RGB for luminance images. For monochromatic, only values are grey values
   - There are two types of color: Additive and Subtractive. Additive is formed via RGBs. Subtractive is via filtering white light with Cyan, Magenta and Yellow
3. Viewer
4. Material

#+NAME: Pinhole camera
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_2.png]]

#+NAME: Synthetic camera
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_3.png]]

This paradigm looks at creating a computer generated image as being similar to forming an image using an optical system.

*** Advantages of Synthetic Camera system
The advantages of such synthetic camera model is the separation of objects viewers and light sources -> simpler API.

Image is also not moved to infront of the camera

** Lighting
Lighting of objects cannot be calculated independently as light can reflect, and some objects are also blocked from light or are translucent.

** Image Formation Model

The API mimics the synthetic camera model to design graphics hardware & software.

For the API, we only need to specify: Objects, materials, viewer and lights

*** Physical approaches
Ray Tracing - follow rays of light from center of projection until they are absorbed by objects or go off course. It can handle global effects but is slow and must keep whole database available

Radiosity - Energy based approach that is slow and not general.

*** Practical Approach for Real-Time rendering
#+NAME: Pipeline architecture
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_4.png]]

All this steps can be implemented in graphics hardware

**** Vertex Processing
Converting object representations from one coordinate system to another (object, synthetic camera, screen coords) via matrix transformations. This is because our 3D scene is described by polygons, which in turn is described by their vertices.

Vertex processor also computes lighting.

**** Projection
This is the process that combines the 3D viewer with the 3D objects to produce 2D image.

Two kinds: Perspective projection where all projectors meet at the center of the projection.
Parallel where the projectors are parallel and center of projection is replaced with a direction of projection.

Part of projection is done by the vertex processor.

**** Primitive Assembly
Vertices must be collected into geometric objects and primitives before clipping and rasterization can take place. So stuff like lines, polygons, curves

**** Clipping
A camera can only see the part of the world within the object space, so things not within this space are clipped out via a series of panes via modifying of the polygons if necessary.

#+NAME: Clipping
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_5.png]]

**** Rasterization
Applies color to the appropriate pixels in the frame buffer aka filling in the interior.

For each object, the rasterizer produces a set of *fragments* for each object/primitive. *Fragments* are potential pixels and they have a location in frame buffer, color and depth attributes.

Vertex attributes are interpolated over the vertex objects by the rasterizer

**** Fragment Processing
Fragments are processed to determine color of corresponding pixel in frame buffer. Colors are determined by *texture mapping* or interpolation of vertex colors.

Fragments can also be blocked by fragments closer to camera and this is removed (*hidden surface removal*)

*** Programmer's interface
This system is exposed via the graphics library API

It exposes functions that specify what is needed to form an image

- Objects
- Viewer
- Light Source
- Material
- Other info like device input and system capabilities

APIs also support a limited set of primitives like points (0D), lines (1D), polygons (2D), curves and surfaces. All these are defined through locations in space or vertices.

#+BEGIN_SRC cpp
glBegin(GL_POLYGON); // type of object
  glVertex3f(0.0, 0.0, 0.0); // location of vertex
  glVertex3f(0.0, 1.0, 0.0);
  glVertex3f(0.0, 0.0, 1.0);
gl.End(); // end of obj definition
#+END_SRC

*** Camera Specification
#+NAME: Camera Specification
#+ATTR_ORG: :width 600
[[file:images/cs3241_l1_6.png]]

*** Lights and Materials
Types of light and material property (absorption, diffuse vs specular scattering) also matter

* Lecture 2 :OpenGL:
OpenGL is a library that mainly focuses on rendering.

It however does not have windowing, support for window systems was added by GLUT.

** Open GL Functions
- Primitives (Points, lines, polygons)
- Attributes
- Transformations (Viewing, Modelling)
- Control (GLUT)
- Input (GLUT)
- Query
 
However, it lacks OOP support so there are often multiple functions that do the same thing.

#+BEGIN_SRC cpp
// All the same
glVertex3f
glVertex2i
glVertex3dv
#+END_SRC

** State
OpenGL is a state machine and exposes two types of functions:
1. Primitive generating - Generates new output if primitive visible
2. State changing via transformation or attribute functions.

Attributes like color, size, polygon mode are part of state and affect object appearance.

** Sample Program

#+TITLE: Square on Solid background
#+BEGIN_SRC cpp
#include <GL/glut.h>

void mydisplay() {
  glClear(GL_COLOR_BUFFER_BIT);
  glBegin(GL_POLYGON);
    glVertex2f(-0.5, -0.5);
    glVertex2f(-0.5, 0.5);
    glVertex2f(0.5, 0.5);
    glVertex2f(0.5, -0.5);
  glEnd();
  glFlush();
}

int main(int argc, char **argv) {
  glutCreateWindow("simple");
  glutDisplayFunc(mydisplay);
  glutMainLoop();
}
#+END_SRC

Program defineds a display callback function mydisplay. Every GLUT program must have display callback and it is executed
everytime the display is refreshed. Main function ends with program entering *event loop*.

However, a better structure would involve having 3 functions:
1. main() - Defines callbacks, open windows, enters event loop
2. init() - Sets state variables like attributes
3. callbacks - display callback function, have input and window functions

*NB:* glOrtho(x1, x2, y1, y2, z1, z2) defines the range of viewing volume, used for clipping.

** Coordinate systems
The units in /glVertex/ are determined by application and are called object coordinates. OpenGL will convert world coords to camera coords and then to window coords

OpenGL camera is at worldspace origin, looking in negative z-direction. Viewing volume is default centered at origin with side of length 2.

Apart from windows, we can use smaller *viewports*, like /glViewport(x, y, w, h)/. Values are in window coords.

** Transformations and viewing
In OpenGL, projection is carried out by the projection matrix.

** Polygon issues
OpenGL only correctly display polygons if simple, convex (all points on line segment between two points in polygon are in polygon), and are flat.

Triangles satisfy all conditions. If not satisfy, may display incorrectly.

** RGB color
Each color component (R / G / B) is stored separately in frame buffer, usually with 8 bits. Color is part of state and is not part of object, rather assigned when object is rendered.
