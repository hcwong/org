#+TITLE: Papers Read
#+AUTHOR: Joshua Wong

* Inflationary Constant Factors and Why Python is Faster Than C++ :PL:
Author: Mehrdad Niknami (UC Berkeley)
Date Read: 6 March 2021

** Impressions
A fairly brief paper showing how certain operations in languages, if poorly designed, could result in
drastic performance outcomes in certain scenarios, mainly in cases of graph algorithms.

** Content
The example used was operators performing comparisons, which is used because most data structures require
some form of partial or total ordering.

Comparisons usually take 2 forms: a 2 way comparison or a 3 way comparison. 3 way comparisons can tell us
if an object is less than or equal to another in a single call to the comparison function.
On the other hand, a 2 way comparison function requires two calls.
#+BEGIN_SRC
def cmp_2way(a, b):
    if a < b:
       return -1
    if b < a:
       return 1
    return 0
#+END_SRC

Therefore, for 2 way comparisons, more than 1 call is made. In certain situations,
such as in a tree data structure, this extra pass/call could lead to an exponential runtime
when performing equality checks.
This is because taking the tree example, at every level of the tree, we are forced to do 2 comparison
calls for each node, and each node needs to do 2 comparison calls for each of its children and so on
and so forth.

This presents interesting problems for languages like C++17 and Python3 which only require
2 way comparison operators like ~<~ or ~__lt__~ whereas in Python2 and C++20, 3 way operators like
~CompareTo~ and ~<=>~ are used instead.

So in this situation, Python2 will perform better than C++! The author also presents more scenarios
in which this exponential slowdown occurs, such as in hashing.

** Implications and usefulness
The implications of this study is mainly on the design of libraries. While we want to keep
implementations relatively unconstrained for developer ease of use, the lack of restrictions might
result in unexpected slowdowns.

Malicious users may also be able to affect application runtimes using this if they are able
to craft malicious inputs of the forms mentioned earlier, resulting in DDOS attacks.

* Google File System
[[http://static.googleusercontent.com/media/research.google.com/en/us/archive/gfs-sosp2003.pdf][Link to paper here]]

** First readthrough
*** Decisions
**** Component Failures are the norm
Recovery, monitoring and fault tolerance are key
**** Files are big
Optimize for file sizes in the GB range rather than KB range.
Design assumptions and parameters such as block sizes have to be revisited.
Small files must be supported but not optimized for
**** Append is the most common operation
Optimise for appending data at the end of files as it is the most
common operation apart from reading.
**** Co-designing aplpications and File System API increases system flexibility.
*** Other assumptions
- Workload consists of two kinds of reads: large streaming reads and smaller individual operations.
- For smaller reads, we can batch and sort to do a one pass read of file.
- Large sequential writes should be optimised for.
- System must implement well defined semantics for multiple writes to same file.
  This is so as to keep synchronization overhead minimal.
- High bandwidth > low latency
*** Interface
Tree structure, with the common file operations. However, there also exist snapshot and record append.
Snapshot copies a file/directory quickly, while record append allows for multiple clients
to write to same file concurrently while guaranteeing atomicity.
*** Architecture
There is a single master (single point of failure?) and multiple chunkservers
#+ATTR_ORG: :width 400
[[file:images/gfs_paper_1.png]]

Chunkservers store chunks of files as Linux files. Each chunk is given a unique 64 bit chunk id.
This id is immutable and is assigned by the master. Each chunk is also replicated for reliability.

Master will maintain all file system metadata. Clients interact with the master for metadata operations
but the data bearing operations go to the chunkservers.

No caching is provided, except for the linux buffer cache in the various chunkservers.

Chunk size is 64MB. Internal fragmentation may be a problem but there is lazy space allocation.

There are some benefits due to a large chunk size
Clients need not interact with master that often because client can cache all the chunk info location from
master, even for a multi TB dataset.
Network overhead is also reduced because a lot of operations can be done on a chunk using a persistent TCP
connection.

*** Chunk locations and log
Master does not have a persistent record of what each chunk server contains.
It just polls them at startup and periodically, and then it stores this metadata in memory.

The operation log is a record of critical metadata changes. This is the only
persistent record of metadata and gives a logical ordering of concurrent operations.

This is stored reliably and changes are not visible to client until the metadata changes
are written to disk. The log is also replicated on multiple machines for security.

Master uses the operation log to startup, so this is kept small. Once the log goes beyond a certain size,
the master will create a checkpoint in another thread from which it can load from.

*** Consistency Model
GFS uses a relaxed consistency model.

File namespaces mutations are atomic because there are handled exclusively by the master.
Namespace locking guarantees atomicity and correctness.

#+ATTR_ORG: :width 400
[[file:images/gfs_paper_2.png]]

A file region is consistent if all clients will always see the same data.
A region is defined after a file data mutation if it is consistent and clients will
see what the mutation writes in its entirety.
Concurrent successful mutations leave the region undefined but consistent. this is because
clients all see the same data but some mutations fail and make the region inconsistent and undefined.

Data mutations may be writes or record append. A write causes data to be written to an
application-specified offset while a record append causes data to be appended atomically at least once to an
offset of GFS's choosing.
The offset a returned to the client and marks the beginning of a defined region that contains the record.
GFS may insert padding in between into inconsistent regions.

After a sequence of successful mutations, the mutated file region is guaranteed to be defined and
contain the data written to by last mutation.
